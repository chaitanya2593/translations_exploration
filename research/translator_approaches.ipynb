{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas\n",
    "import numpy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| **Approach**                     | **Data Required**      | **Overview**                                                                                             | **Strengths**                       | **Challenges**                    | **References**                                                                                   |\n",
    "|-----------------------------------|------------------------|----------------------------------------------------------------------------------------------------------|-------------------------------------|------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **Zero-Shot Translation**         | None                  | Leverage multilingual models trained on large corpora to perform translation without explicit parallel data. | Fast, multilingual support          | May lack domain-specific accuracy | [M2M-100 by Facebook](https://arxiv.org/abs/2010.11125), [XLM](https://arxiv.org/abs/1901.07291) |\n",
    "| **Back-Translation**              | Monolingual           | Create synthetic parallel data by translating from the target language back to the source language.       | Generates synthetic parallel data   | Translation quality impacts model | [Unsupervised Machine Translation](https://arxiv.org/abs/1804.07755), [MarianMT](https://marian-nmt.github.io/) |\n",
    "| **Denoising Autoencoders**        | Monolingual           | Train models to reconstruct noisy text, aligning representations across languages without parallel data. | Fully unsupervised                  | Computationally expensive         | [Unsupervised NMT](https://arxiv.org/abs/1710.11041), [XLM Model](https://arxiv.org/abs/1901.07291) |\n",
    "| **Cross-Lingual Embedding Alignment** | Monolingual       | Train embeddings separately for each language and align them in the same semantic space for translation. | Lightweight, effective for keywords | Limited to word/phrase-level      | [MUSE by Facebook](https://github.com/facebookresearch/MUSE), [FastText](https://fasttext.cc/)   |\n",
    "| **Unsupervised Pre-Trained Models** | Monolingual         | Use pre-trained models like T5 or GPT, fine-tuned for unsupervised or specific language tasks.            | Robust pre-trained capabilities     | May need fine-tuning              | [T5](https://arxiv.org/abs/1910.10683), [mT5](https://arxiv.org/abs/2010.11934)                 |\n",
    "| **Iterative Back-Translation**    | Monolingual           | Use back-translation iteratively to improve synthetic data and model performance.                        | Incremental improvement             | Time-consuming process            | [Improving Back-Translation](https://arxiv.org/abs/1805.08241)                                  |\n",
    "| **Data Augmentation**             | Monolingual           | Generate synthetic data using paraphrasing or augmentation to improve the training dataset size.         | Expands training data               | Noise in generated data           | [Paraphrasing for Translation](https://arxiv.org/abs/1909.13838), [Text Augmentation](https://github.com/jasonwei20/eda_nlp) |\n",
    "| **Adversarial Training**          | Monolingual           | Use adversarial methods to align representations across languages, enabling unsupervised translation.    | Fully unsupervised                  | Complex to implement              | [Adversarial Alignment for NMT](https://arxiv.org/abs/1706.05075), [GANs in NMT](https://arxiv.org/abs/1703.04887) |\n",
    "| **Multilingual Fine-Tuning**      | Monolingual           | Fine-tune multilingual pre-trained models (e.g., XLM-R, mT5) on domain-specific monolingual data.        | Adaptable to specific domains       | Needs computing resources         | [XLM-R](https://arxiv.org/abs/1911.02116), [mT5](https://arxiv.org/abs/2010.11934)              |\n",
    "| **Active Learning**               | Minimal labeled data  | Dynamically label the most useful examples to train a model efficiently with minimal supervision.         | Efficient labeling process          | Needs active learning pipeline    | [Active Learning for NMT](https://arxiv.org/abs/1706.08500), [AL in NLP](https://arxiv.org/abs/2006.11477) |\n"
   ],
   "id": "adf9a9bee23f7e3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "** The above listed approaches are recommended BY Chat GPT and they are pretty much around 2017-2020 time. This approaches are obsolate considering the latest approaches we have with the LLMs liek GPT 4.\n",
   "id": "4bb67b0dd61e37e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LATEST APPROACHES\n",
    "| **Approach**                        | **Data Required**      | **Overview**                                                                                          | **Strengths**                          | **Challenges**                    | **References**                                                                                     |\n",
    "|--------------------------------------|------------------------|-------------------------------------------------------------------------------------------------------|----------------------------------------|------------------------------------|----------------------------------------------------------------------------------------------------|\n",
    "| **Prompt-Based Learning**            | None or Monolingual    | Uses LLMs (e.g., GPT-4, PaLM-2) with task-specific prompts for translation. Requires minimal or no fine-tuning. | Zero-shot/few-shot capability, domain flexibility | Requires access to large LLMs      | [InstructGPT](https://arxiv.org/abs/2203.02155), [PaLM-2](https://ai.google/static/palm/)          |\n",
    "| **Adapter-Based Fine-Tuning**        | Monolingual or Limited Parallel | Fine-tune only small parts (adapters) of massive multilingual models like mT5 or XLM-R.              | Lightweight, efficient for updates      | Fine-tuning still required          | [Adapters for mT5](https://arxiv.org/abs/2110.04366), [PELT for NLP](https://arxiv.org/abs/2208.05581) |\n",
    "| **Multilingual In-Context Learning** | None or Monolingual    | Train LLMs to support context-sensitive translations across languages without parallel data.          | No task-specific fine-tuning required   | Sensitive to input prompt design    | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)                                         |\n",
    "| **Efficient Transformers (e.g., LongFormers)** | Monolingual or Limited Parallel | Focus on long-context translation tasks using efficient transformer architectures.                   | Handles long texts efficiently           | Requires architectural adaptations  | [LongFormer](https://arxiv.org/abs/2004.05150), [BigBird](https://arxiv.org/abs/2007.14062)         |\n",
    "| **Contrastive Learning for Translation** | Monolingual           | Uses contrastive objectives to improve representation alignment for multilingual embedding spaces.    | Improves cross-lingual performance       | Resource-intensive training         | [InfoXLM](https://arxiv.org/abs/2101.08296), [CLIP for Text](https://arxiv.org/abs/2103.00020)      |\n",
    "| **Sparse Models (Mixture of Experts)** | Monolingual           | Mixture-of-Experts (MoE) architectures dynamically activate parts of the model for specific tasks.   | Scales efficiently with large models     | Complex implementation              | [Switch Transformers](https://arxiv.org/abs/2101.03961), [GLaM](https://arxiv.org/abs/2112.06905)   |\n",
    "| **Self-Supervised Cross-Alignment**  | Monolingual           | Align representations across languages using self-supervised pre-training, avoiding parallel data.   | No parallel data needed, effective       | Requires substantial compute power  | [XLM-E](https://arxiv.org/abs/2204.10487), [ParaMAE](https://arxiv.org/abs/2205.00330)              |\n",
    "| **Knowledge Distillation for MT**    | Monolingual           | Train smaller, efficient translation models using LLMs as teachers to distill knowledge.             | Reduces size and complexity of models    | Requires large teacher models       | [Distilling Translation Models](https://arxiv.org/abs/2212.07677)                                  |\n",
    "| **Retrieval-Augmented Translation**  | Monolingual           | Combines neural translation with retrieval systems to improve performance on domain-specific tasks.  | Domain adaptability, better accuracy     | Requires a retrieval database setup | [RETRO](https://arxiv.org/abs/2112.04426), [RAG](https://arxiv.org/abs/2005.11401)                  |\n",
    "| **Hybrid Neural-Symbolic MT**        | Monolingual           | Combine neural MT with rule-based or symbolic reasoning for enhanced translation quality.            | Better explainability and accuracy       | Hybrid design complexity            | [Neuro-Symbolic AI](https://arxiv.org/abs/2301.02177), [Symbolic MT Approaches](https://arxiv.org/abs/2209.11762) |\n"
   ],
   "id": "16943f9e65b12339"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluatuon of the Machine Tranlation\n",
    "\n",
    "There are many metrics which we can use for the machine translation. But we need to choose the metrics which are more into the semantic similarity and also have some domain knowledge.\n",
    "\n",
    "Metric\tDescription\n",
    "- BLEU (Bilingual Evaluation Understudy)\tMeasures n-gram overlap between the machine translation and reference translation. Higher scores indicate closer alignment with human references. It is widely used but can struggle with synonyms or paraphrasing.\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\tFocuses on recall-based overlap of n-grams, primarily used in summarization but applicable to MT. ROUGE-L specifically considers sequence alignment. It is helpful for evaluating fluency and coverage.\n",
    "- METEOR (Metric for Evaluation of Translation with Explicit ORdering)\tConsiders exact matches, stemming, synonyms, and paraphrases between translation and reference. It uses precision, recall, and a harmonic mean to give a more human-like judgment. It is better at capturing semantic similarity than BLEU.\n",
    "- TER (Translation Edit Rate)\tCalculates the number of edits (insertions, deletions, substitutions, and shifts) needed to make a machine translation match a reference. Lower TER scores mean fewer corrections are needed, reflecting better quality.\n",
    "- chrF (Character F-score)\tEvaluates translation quality at the character level instead of word level, making it suitable for morphologically rich languages. It combines precision and recall to account for partial matches, such as prefixes or suffixes.\n",
    "- COMET (Cross-lingual Optimized Metric for Evaluation of Translation)\tUses a neural network trained on human judgment to predict translation quality. COMET evaluates translations on adequacy, fluency, and consistency, making it more robust than traditional metrics.\n",
    "- BERTScore\tCompares contextual embeddings of the machine and reference translations using pre-trained models like BERT. It captures semantic similarity and paraphrasing better than n-gram-based metrics.\n",
    "- BLEURT (Bilingual Evaluation Understudy with Representations from Transformers)\tA transformer-based metric fine-tuned on human ratings, designed to predict translation quality. BLEURT captures semantic differences and aligns well with human judgments.\n",
    "- SacreBLEU\tA standardized version of BLEU with consistent preprocessing and tokenization, ensuring reproducibility across evaluations. It is simpler to use than traditional BLEU but shares similar limitations.\n",
    "- Human Evaluation\tThe gold standard, involving human raters who evaluate adequacy, fluency, and overall translation quality. It is costly and time-consuming but essential for benchmarking against automated metrics.\n",
    "\n",
    "Well Human Evaluation is the last step so we need to focus on the other aspects. THe BLEU , ROUGE, TER, chrF n Meteor are more into the Frequency based approach.  Amoung the Transformer based approaches we can look into the BLEURT, COMET, SBLEU/DBLEU\n",
    "\n",
    "\n",
    "Recommendation for our Z+ Case\n",
    "- Start with Pre-trained BLEURT: Test the general pre-trained model to see how well it handles our translations.\n",
    "- Fine-tune if Necessary: If the BLEURT scores are not aligned with human judgments (e.g., it fails to recognize Z+ e-commerce-specific terms or paraphrasing), fine-tune the model using a smaller, domain-specific dataset. I am not sure we have any specific dataset inside Z+ for this.\n",
    "- A few thousand high-quality parallel sentences for fine-tuning are often sufficient.\n",
    "Ensure High-Quality Reference Data: Regardless of the approach, your dataset quality (e.g., accuracy and fluency of reference translations) is key to improving performance.\n",
    "\n",
    "For BLEU we need a reference TEXT which will be actual text we are comparing and the Candiate which will be the translated text in our case.\n",
    "\n",
    "\n"
   ],
   "id": "b689cebccd4ec82a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BLEU SCORES RANGE\n",
    "| **BLEU** | **Interpretation**                                      |\n",
    "|----------|---------------------------------------------------------|\n",
    "| < 0.1    | Almost useless                                          |\n",
    "| 0.1-0.19 | Hard to get the gist                                    |\n",
    "| 0.2-0.29 | The gist is clear, but has significant grammatical errors|\n",
    "| 0.3-0.39 | Understandable to good translations                     |\n",
    "| 0.4-0.49 | High quality translations                               |\n",
    "| 0.5-0.59 | Very high quality, adequate, and fluent translations    |\n",
    "| â‰¥ 0.6    | Quality often better than humans                        |\n",
    "\n",
    "Ref: https://codelabsacademy.com/en/blog/understanding-bleu-score-in-nlp-evaluating-translation-quality"
   ],
   "id": "dd16229ab0f5d48c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BLURT/COMET scores typically range from 0 to 1, where:\n",
    "\n",
    "- 0 - 0.2: Poor quality\n",
    "- 0.2 - 0.4: Fair quality\n",
    "- 0.4 - 0.6: Good quality\n",
    "- 0.6 - 0.8: Very good quality\n",
    "- 0.8 - 1.0: Excellent quality2"
   ],
   "id": "885ff7c10633b72e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
